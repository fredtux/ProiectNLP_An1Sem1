{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "[Tutorial: Working with Hugging Face Models and Datasets](https://github.com/anyuanay/medium/blob/main/src/working_huggingface/Working_with_HuggingFace_ch4_Fine_Tuning_Pretrained_Model_for_Question_Answering.ipynb)\n",
    "\n",
    "[Fine-Tuning-for-Question-Answering-SQuAD-IndoQA](https://github.com/PrasetyoWidyantoro/Fine-Tuning-for-Question-Answering-SQuAD-IndoQA/blob/master/fine-tune-squad-dataset.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "model_name = \"distilbert-base-multilingual-cased\"\n",
    "model_short_name = \"distilbert-multilingual\"\n",
    "\n",
    "dataset_name = (\"xquad\", \"xquad.ro\")\n",
    "dataset_short_name = \"xquad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952 119 119\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(dataset_name[0], dataset_name[1])\n",
    "\n",
    "ds = ds.shuffle(seed=random_seed)\n",
    "ds = ds['validation'].train_test_split(test_size=0.2)\n",
    "ds_train = ds['train']\n",
    "ds_test = ds['test']\n",
    "\n",
    "ds_test = ds_test.train_test_split(test_size=0.5)\n",
    "ds_val = ds_test['train']\n",
    "ds_test = ds_test['test']\n",
    "\n",
    "print(len(ds_train), len(ds_val), len(ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(text: str):\n",
    "    return text\n",
    "    # return text.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\").replace(\"Ţ\", \"Ț\").replace(\"Ş\", \"Ș\")\n",
    "\n",
    "def process_and_tokenize(dataset: Dataset, tokenizer: AutoTokenizer) -> dict:\n",
    "    # Extract from the dataset and standardize where possible\n",
    "    questions = [standardize(q).strip() for q in dataset[\"question\"]]\n",
    "    contexts = [standardize(c) for c in dataset[\"context\"]]\n",
    "    answers = dataset[\"answers\"]\n",
    "    \n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = inputs[\"overflow_to_sample_mapping\"]\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    \n",
    "    start_pos = []\n",
    "    end_pos = []\n",
    "\n",
    "    for i, ofs in enumerate(offset_mapping):\n",
    "        # Get the sample\n",
    "        sample_i = sample_mapping[i]\n",
    "        \n",
    "        # Get the answer for the sample\n",
    "        answer = answers[sample_i]\n",
    "        \n",
    "        # Get the start and end character of the answer\n",
    "        start = answer[\"answer_start\"][0]\n",
    "        end = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        \n",
    "        # Get the sequence ids\n",
    "        seq_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Get the start and end token positions\n",
    "        start_context = seq_ids.index(1)\n",
    "        end_context = next(j - 1 for j in range(start_context, len(seq_ids)) if seq_ids[j] != 1)\n",
    "\n",
    "        if ofs[start_context][0] > start or ofs[end_context][1] < end: # If it's impossible\n",
    "            start_pos.append(0)\n",
    "            end_pos.append(0)\n",
    "        else: # Get and append start and end position\n",
    "            start_pos.append(next((j - 1 for j in range(start_context, end_context + 1) if ofs[j][0] > start), end_context))\n",
    "            end_pos.append(next((j + 1 for j in range(end_context, start_context - 1, -1) if ofs[j][1] < end), start_context))\n",
    "            \n",
    "            # if i < len(dataset['id']) and dataset['id'][i] == '56d6f3500d65d21400198292':\n",
    "            #     print(start_context, end_context, end_pos[-1], end_pos[-1])\n",
    "\n",
    "    inputs[\"start_positions\"] = start_pos\n",
    "    inputs[\"end_positions\"] = end_pos\n",
    "        \n",
    "    inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    inputs.pop(\"offset_mapping\")\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76a50f0ea7848078c62d84c8c9ab491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082\n"
     ]
    }
   ],
   "source": [
    "ds_tok_train = ds_train.map(lambda x: process_and_tokenize(dataset=x, tokenizer=tokenizer), batched=True, batch_size=64, remove_columns=ds_train.column_names)\n",
    " \n",
    "print(len(ds_tok_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_tokenize_val_test(dataset: Dataset, tokenizer: AutoTokenizer) -> dict:\n",
    "    # Extract from the dataset and standardize where possible\n",
    "    questions = [standardize(q).strip() for q in dataset[\"question\"]]\n",
    "    contexts = [standardize(c) for c in dataset[\"context\"]]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = inputs[\"overflow_to_sample_mapping\"]\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_i = sample_mapping[i]\n",
    "        example_ids.append(dataset[\"id\"][sample_i])\n",
    "\n",
    "        seq_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        \n",
    "        for j, ofs in enumerate(offset):\n",
    "            inputs['offset_mapping'][i][j] = ofs if seq_ids[j] == 1 else None\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    \n",
    "    inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4f23696ebe42b98e5d15a974cb45da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40fc9815fa74f5bb51dd1c43db29ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 143\n"
     ]
    }
   ],
   "source": [
    "ds_tok_val = ds_val.map(lambda x: process_and_tokenize_val_test(dataset=x, tokenizer=tokenizer), batched=True, batch_size=64, remove_columns=ds_val.column_names)\n",
    "ds_tok_test = ds_test.map(lambda x: process_and_tokenize_val_test(dataset=x, tokenizer=tokenizer), batched=True, batch_size=64, remove_columns=ds_test.column_names)\n",
    " \n",
    "print(len(ds_tok_val), len(ds_tok_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_metric = evaluate.load(\"squad\")\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits: list, end_logits: list, tok: dict, dataset: Dataset, max_answer_length: int = 80) -> dict:\n",
    "    extracted_features = {}\n",
    "    \n",
    "    # Group extracted features by ids\n",
    "    for i, feature in enumerate(tok):\n",
    "        if feature[\"example_id\"] not in extracted_features:\n",
    "            extracted_features[feature[\"example_id\"]] = [i]\n",
    "        else:\n",
    "            extracted_features[feature[\"example_id\"]].append(i)\n",
    "    \n",
    "    golds_squad = [{\"id\": data[\"id\"], \"answers\": data[\"answers\"]} for data in dataset]\n",
    "    preds_squad = []\n",
    "    \n",
    "    golds_bleu = []\n",
    "    preds_bleu = []\n",
    "    \n",
    "    for data in tqdm(dataset):\n",
    "        answers = []\n",
    "\n",
    "        # Iterate over all the extracted features\n",
    "        for i in extracted_features[data[\"id\"]]:\n",
    "            start_logit = start_logits[i]\n",
    "            end_logit = end_logits[i]\n",
    "            offs = tok[i][\"offset_mapping\"]\n",
    "\n",
    "            # Get all combinations of start and end positions\n",
    "            for start_i in range(len(start_logit)):\n",
    "                for end_i in range(len(end_logit)):\n",
    "                    # Continue on wrong answers\n",
    "                    if offs[start_i] is None \\\n",
    "                        or offs[end_i] is None \\\n",
    "                        or end_i < start_i \\\n",
    "                        or end_i - start_i + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    \n",
    "                    # Add text and score\n",
    "                    answer = {\n",
    "                        \"answer\": data[\"context\"][offs[start_i][0] : offs[end_i][1]],\n",
    "                        \"score\": start_logit[start_i] + end_logit[end_i]\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "                    \n",
    "        preds_squad.append(\n",
    "            {\"id\": data['id'], \"prediction_text\": max(answers, key=lambda x: x[\"score\"])['answer']} \n",
    "                if len(answers) > 0 else \n",
    "                {\"id\": data['id'], \"prediction_text\": \"\"}\n",
    "        )\n",
    "        \n",
    "        preds_bleu.append(\n",
    "            max(answers, key=lambda x: x[\"score\"])['answer']\n",
    "                if len(answers) > 0 else \n",
    "                \"\"\n",
    "        )\n",
    "        \n",
    "        golds_bleu.append(*[ans for ans in data[\"answers\"]['text']])\n",
    "        \n",
    "    return {\n",
    "        \"squad\": squad_metric.compute(predictions=preds_squad, references=golds_squad),\n",
    "        \"bleu\": bleu_metric.compute(predictions=preds_bleu, references=golds_bleu),\n",
    "        \"rouge\": rouge_metric.compute(predictions=preds_bleu, references=golds_bleu)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, evaluation and testing loop for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_metrics_to_file(metrics: dict, metric_type: str = None, lr: float = None, epoch: int = None):\n",
    "    filename = os.path.join(\"results\", f\"{model_short_name}-{dataset_short_name}-type_{metric_type}-lr_{lr}-epoch_{epoch:02}.json\" if lr is not None and epoch is not None and metric_type is not None else f\"{model_short_name}-{dataset_short_name}.json\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "        \n",
    "def save_model(model: AutoModelForQuestionAnswering, lr: float, epoch: int):\n",
    "    model.save_pretrained(os.path.join(\"results\", \"model\", f\"{model_short_name}-{dataset_short_name}-{lr}-{epoch}\"))\n",
    "    \n",
    "def load_model(lr: float, epoch: int) -> AutoModelForQuestionAnswering:\n",
    "    return AutoModelForQuestionAnswering.from_pretrained(os.path.join(\"results\", \"model\", f\"{model_short_name}-{dataset_short_name}-{lr}-{epoch}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get maximum answer length within 2 standard deviations from the mean of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_answer_length = np.mean([len(a[\"text\"][0]) for a in ds_train[\"answers\"]])\n",
    "std_dev_answer_length = np.std([len(a[\"text\"][0]) for a in ds_train[\"answers\"]])\n",
    "\n",
    "two_std_devs_above = mean_answer_length + 2 * std_dev_answer_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tet_loop(lr_list: list, epochs_list: list) -> None:\n",
    "    all_models_eval = {\"validation\": {lr:{epochs:[] for epochs in epochs_list} for lr in lr_list}, \"test\": {lr:{epochs:[] for epochs in epochs_list} for lr in lr_list}}\n",
    "    \n",
    "    for lr in lr_list:\n",
    "        for epochs in epochs_list:\n",
    "            print(f\"Training with lr={lr} and epochs={epochs}\")\n",
    "            args = TrainingArguments(\n",
    "                output_dir=\"./results\",\n",
    "                eval_strategy=\"no\",\n",
    "                save_strategy=\"epoch\",\n",
    "                learning_rate=lr,\n",
    "                num_train_epochs=epochs,\n",
    "                weight_decay=0.01,\n",
    "                per_device_train_batch_size=8, \n",
    "            )\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=args,\n",
    "                train_dataset=ds_tok_train,\n",
    "                eval_dataset=ds_tok_val,\n",
    "                processing_class=tokenizer\n",
    "            )\n",
    "            trainer.train()\n",
    "            \n",
    "            preds, _, _ = trainer.predict(ds_tok_val)\n",
    "            start_logits, end_logits = preds\n",
    "            computed_metrics = compute_metrics(start_logits, end_logits, ds_tok_val, ds_val, two_std_devs_above)\n",
    "            \n",
    "            output_metrics_to_file(computed_metrics, metric_type='validation', lr=lr, epoch=epochs)\n",
    "            all_models_eval[f\"{lr}-{epochs}\"] = computed_metrics\n",
    "            \n",
    "            preds, _, _ = trainer.predict(ds_tok_test)\n",
    "            start_logits2, end_logits2 = preds\n",
    "            computed_metrics = compute_metrics(start_logits2, end_logits2, ds_tok_test, ds_test, two_std_devs_above)\n",
    "            \n",
    "            output_metrics_to_file(computed_metrics, metric_type='test', lr=lr, epoch=epochs)\n",
    "            \n",
    "            save_model(model, lr, epochs)\n",
    "            \n",
    "            print(f\"Finished training, validating and testing with lr={lr} and epochs={epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tet_loop([1e-5, 1e-4, 1e-3], [2, 4, 8, 16])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
